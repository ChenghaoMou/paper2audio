
    <!DOCTYPE html>
    <html>
    <head>
    <link rel="stylesheet" href="https://unpkg.com/latex.css/style.min.css" />
    </head>
    <body>
    <h1 class='title'>Orion-14B: Open-source Multilingual Large Language Models</h1><h2 class='section-header'>Abstract</h2><p class='text'>In this study, we introduce Orion-14B, a collection of multilingual large language models with 14 billion parameters. We utilize a data scheduling approach to train a foundational model on a diverse corpus of 2.5 trillion tokens, sourced from texts in English, Chinese, Japanese, Korean, and other languages. Additionally, we fine-tuned a series of models tailored for conversational applications and other specific use cases. Our evaluation results demonstrate that Orion-14B achieves state-of-the-art performance across a broad spectrum of tasks. We make the Orion- 14B model family and its associated code publicly accessible1, aiming to inspire future research and practical applications in the field.</p><h2 class='section-header'>1 Introduction</h2><p class='text'>Three hundreds years ago, Gottfried Wilhelm Leibniz’s insightful declaration that "Language is the mirror of the mind" profoundly resonates in the contemporary exploration of language. This thought provides a philosophical foundation for understanding the intricate relationship between language and intelligence. In the 20th century, language modeling (LM) became a fundamental approach in artificial intelligence, forming the cornerstone of natural language processing (NLP). The goal of language modeling is to learn the probability distribution of word sequences. Desipite its simple modeling procedure, it encapsulates substantial information about languages. Given that a language contains N words, the potential number of word sequences of the length of L is N L. However, the actual number of sentences commonly used in the language is far fewer than N L. This discrepancy highlights how language models effectively encode linguistic information.</p><p class='text'>Traditionally, statistical methods were employed to model word frequency in a language. Among these, the N-gram model has been widely adopted, determining the probability of a word based on the previous N − 1 words. Though straightforward and efficient, the method suffers from the data sparsity problem. With the advancement of neural networks, a paradigm shift occurred towards employing neural networks for language modeling. There are many variations of neural language models, such as multi-layer perceptron (MLP) , recurrent neural networks (RNN) , and transformer .</p><p class='text'>In recent years, the increase of model sizes and the scale of training data have significantly boosted the capability of language models . Large language models (LLMs) have exhibited remarkable promise in many traditional NLP tasks, such as dialogue system, machine translation, information retrieval. Moreover, LLMs have proven adept at complex tasks such as reasoning, code generation, creative writing. These advancements have inspired both the academic and industrial sectors to further investigate the underlying principles and potential applications of LLMs.</p><p class='text'>The launch of ChatGPT/GPT-3.5  in 2022 captured tremendous attention from the public, pushing the boundaries of what AI can achieve and motivating researchers and engineers to delve deeper into their potential. While GPT-3.5 and its successor, GPT-4 , are prime examples of LLMs, their specific model architectures and training methodologies remain undisclosed. In contrast, Meta’s release of LLaMA  and LLaMA 2  have established a widely-recognized LLM architecture within the open-source community, with numerous libraries adopting these models. Despite LLaMA’s impressive performance, its primary focus on English limits its applicability to other languages. Recently, there has been a surge in the release of multilingual LLMs such as ChatGLM , Baichuan , Qwen , InternLM , XVERSE , Skywork  and Yi . These models, trained on mainly English and Chinese datasets, have shown promising results in tasks involving both English and Chinese. Additionally, there has been a growing trend of LLMs specifically designed to enhance performance in other languages, such as Japanese and Korean .</p><p class='text'>In this technical report, we present Orion-14B, a family of multilingual language models with 14 billion parameters. The foundation model is trained on a diverse dataset of 2.5 trillion (2.5T) tokens, containing languages such as English, Chinese, Japanese, Korean, and others. It has demonstrated superior performance across a broad spectrum of tasks in multilingual settings.</p><p class='text'>We also provides a series of fine-tuned models built upon the foundation model, each trained to different focuses such as conversation, long-context text handling, quantization, and specific application requirements.</p><p class='text'>The remainder of this report describes our data preparation (Section 2), pretraining methodology (Section 3), fine-tuning methodology (Section 4), evaluation results (Section 5), extension works (Section 6), and conclusions (Section 7).</p><h2 class='section-header'>2 Data</h2><p class='text'>In the training framework of LLMs, the role of data is crucial in determining the efficacy and performance of these models. Effective data processing for pretraining is essential for achieving the desired outcomes. This involves acquiring data from diverse sources, ensuring the high quality of the data through thorough filtering, and removing any redundant information. This section will discuss these processes in detail, outlining the necessary steps to prepare and refine data to suit the stringent requirements of LLM training.</p><h2 class='section-header'>2.1 Data Source</h2><p class='text'>Pretraining of LLM needs tremendous amounts of data. offered guildlines regarding the optimal quantity of training data for models of varying sizes. For example, an LLM with 10 billion parameters requires 205 billion tokens for pretraining. However, recent work  in training 10 billion parameter models have utilized 2.5 to 3 trillion tokens, substantially exceeding the recommended data volume. These efforts have yielded notably impressive results, demonstrating the efficacy of training with significantly larger datasets than those proposed in the aforementioned study.</p><p class='text'>In order to obtain such a large amount of data, it is imperative to collect data from multitude of sources with diversity and high quality. Our dataset incorporates texts in multiple languages, with English and Chinese being predominant, constituting over 90% of the entire dataset. Particular efforts are also made to include Japanese and Korean texts, which account for more than 5% of the dataset. The remaining portion comprises texts in various other languages, such as Spanish, French, German, Arabic, and more.</p><p class='text'>In terms of content and style, the dataset primarily composes of written language, with spoken language constituting only a minor portion. The dataset spans a broad spectrum of topics, including web pages, news articles, encyclopedic entries, books, source code, and academic publications, among others. The diversity within the dataset is a crucial factor in achieving superior performance across a range of tasks. The detailed distribution of the data sources is shown in Fig. 1. We believe that different types of corpora exert varying influences on the model training process; for instance, some may be more effective to language understanding, while others better facilitate knowledge reasoning. Unlike many existing studies that typically employ random shuffling of training examples, we strategically feeds the model with varied data sources across different training stages. We believe this method leads to more efficient data usage. The details of this approach will be elaborated in Section 3.</p><p class='caption'>Figure 1: Data sources distribution.</p><h2 class='section-header'>2.2 Data Quality</h2><p class='text'>Data quality is essential in the training of LLMs. To assure high-quality data, we have implemented a series of measures for data filtering, detailed as follows:</p><p class='list-item'>• Text normalization: The datasets contain a large number of texts from various sources, such as web pages and ebooks. These texts are often accompanied by HTML, special characters, or other format tags, which are not useful for LLM training. We employ a series of tools, such as regular expressions and format parsers, to effectively eliminate them.</p><p class='list-item'>• Harmful content removal: The Internet contains harmful and spam content. Our approach to mitigate this involves a two-stage process: the initial stage utilizes keywords and regular expressions matching, followed by a deep learning-based model designed to identify and remove such content. It is important to note that entirely eliminating harmful text from the training dataset could lead to a scenario where the trained model lacks the ability to identify and appropriately response to harmful information . Therefore, we intentionally retain a minimal amount of harmful text in the dataset. This approach ensures that the model remains capable of recognizing and effectively addressing such content.</p><p class='list-item'>• Personal information removal: Some of the text data includes personal details like names, phone numbers, and addresses. We utilize rule-based methods for detection and either substitute these with placeholders or remove them entirely.</p><p class='list-item'>• Quality filtering: This part is crucial in data processing. We first apply a set of rules to filter the data, such as removing texts with excessive repetition. Additionally, we use N-gram perplexity models to exclude texts with anomalously high perplexity. Lastly, our proprietary data quality models are employed to select high-quality data. We emphasize that while high quality is essential for LLM training, achieving a balance between quality and quantity of training data is a non-trivial task. Our models are optimized to retain as much data as possible while maintaining high data quality, which is one of the key factors in the successful training of LLMs.</p><h2 class='section-header'>2.3 Deduplication</h2><p class='text'>Given that the training data for LLMs is sourced from a variety of origins, there is a significant likelihood of encountering duplicate data. Duplicate data can detrimentally affect the training process, potentially leading to a model biased towards certain data sources and a decline in performance . To address this, we develop a deduplication procedure to eliminate redundant data.</p><p class='text'>In this process, we extract key words and phrases from each document and compute their correspond- ing embedding vectors and SimHash vectors . These vectors are then compared to those in our database. If a vector in the database shows similarity within a certain threshold, the document is considered a duplicate and is subsequently discarded.</p><p class='text'>Importantly, we note that while LLMs have shown significant advancements in numerous NLP tasks, some studies  indicate that part of this improvement might be attributed to unintentional inclusion of evaluation data in the training datasets, potentially leading to overestimated results. To address this, we adopt our deduplication approach for all evaluation datasets to prevent the pretraining dataset from containing texts in the evaluation sets, thereby enhancing the integrity and reliability of our model’s evaluation results. We will further discuss the data contamination in detail in Section 5.3.</p><h2 class='section-header'>3 Pretraining 3.1 Tokenizer</h2><p class='text'>A tokenizer is a basic component of an LLM, which need to represent the text distribution in the language while maintaining an favorable vocabulary size for training. For a multilingual tokenizer, statistical methods are typically employed to generate word-level or subword-level tokens from texts in multiple languages. We utilize the byte-pair encoding (BPE) algorithm , implemented via SentencePiece . Our configuration ensures a character coverage of 99.99%, with rare characters defaulting to UTF-8 bytes. To build a diverse corpus and align with our training data distribution, we curate a broad spectrum of text types from our training corpus. This includes English, Simplified Chinese, Traditional Chinese, Japanese, Korean, a few other languages, as well as rare characters. In Table 1, we provide a detailed comparison of our tokenizer with other open-source tokenizers. This comparison includes vocabulary size and compression ratio (CR), the latter calculated by the ratio of the size of the original data to the size of the tokenized data.</p><p class='caption'>Table 1: Tokenizer comparison with other open-source LLMs. We compare vocabulary sizes and compression ratios for simpifiled Chinese (zh_cn), tranditional Chinese (zh_cn), and English, respectively.</p><h2 class='section-header'>3.2 Architecture</h2><p class='text'>Given that LLaMA2 has achieved superior performance, its architecture has been widely adopted by many open-source LLM. In our approach, we adhere to the LLaMA2 architecture while implementing several modifications. These include extending the number of tokens to 84,608 and enlarging the dimensions of the feed-forward network (FFN) to 15,360. We employ rotary positional embeddings (RoPE)  for positional encoding to accommodate context lengths of up to 4096 tokens. The model uses 40 transformer layers with 40 attention heads each. The total parameter of the model is 14.4 billion, slightly exceeding that of LLaMA2-13B. Detailed model parameters is provided in Table 2.</p><h2 class='section-header'>4</h2><p class='caption'>Table 2: A comparison of model architecture. The table shows comparison of our model and several open-source model with similar model size.</p><h2 class='section-header'>3.3 Infrastructure</h2><p class='text'>For the training of Orion-14B, we employed Megatron-LM  on a cluster comprising 11 servers, each equipped with 8 NVIDIA H800 GPUs. To optimize training efficiency, we integrated FlashAttention2  and APEX  into Megatron-LM framework, achieving a training speed of 4,000-5,000 tokens/GPU/second.</p><h2 class='section-header'>3.4 Training Process</h2><p class='text'>To train Orion-14B, we initiate the model training with a learning rate warm-up stage spanning 2000 iterations, during which we linearly increase the learning rate to the maximal learning rate of 3e-4. We then apply a cosine schedule to gradually decrease the learning rate to 3e-5 throughout the training processing. We employ the AdamW  optimizer, setting β1 to 0.9 and β2 to 0.95, respectively. In addition, we apply a weigh decay factor of 0.1 and enforce a gradient clipping threshold of 1.0 to ensure the stability of the training process. The model is trained using BF16/FP32 mixed precision, with a batch size of 1408, corresponding to approximately 5.7 million tokens per step.</p><h2 class='section-header'>3.5 Data Scheduling</h2><p class='text'>Training large language models requires hundreds of billions to trillions of tokens. suggests that model training tends to favor an increase in the number of tokens over model sizes. We use a 2.5T token training dataset for our 14B parameter model, aiming a balance between computational efficiency and cost.</p><p class='text'>On the other side, while numerous theoretical and empirical studies have examined the interplay between model size and training data volume, there is no universally accepted methodology for scheduling training data. Considering that humans acquire knowledge in a deliberate order , it is plausible that language models might also benefit from a structured learning order when processing training data. Curriculum learning  has been suggested as a method to organize the learning process by progressively increasing the complexity of the training data. However, most prior studies have concentrated on sample-level data and smaller datasets. employed a skills-based framework for training data selection and continuous pretraining with a 3B-parameter language model. This approach achieved greater accuracy compared to the baseline method of uniform data source sampling, suggesting the potential efficacy of strategic data scheduling.</p><p class='text'>In training the Orion-14B model, we intentionally develop a data scheduling strategy that organizes training data to incrementally increase its complexity. We divide the training data into several stages based on the data sources and their complexity. These stages are differentiated by the mix ratios of data sources. Initial stages primarily include data with common knowledge, such as web pages and news articles. In the subsequent stages, we gradually augment the proportion of data containing more complex knowledge, including textbooks, academic papers, and source code. Additionally, the linguistic diversity of the training data is expanded progressively from English and Chinese to Japanese and Korean. The brief structure of our training data schedule is depicted in Table 3.</p><p class='text'>To assess the effectiveness of the data scheduling approach, we monitor the loss on a validation set throughout the training process. This validation set consists of 5,000 documents, each unseen in the</p><p class='caption'>Table 3: Training data schedule for Orion-14B. Primary data sources and languages refer to data that totally account for more than 90% of the whole training data in each stage.</p><p class='text'>training set. It includes a diverse collection of English and Chinese texts sourced from a variety of data sources. As shown in Fig. 2, there are significant reduction in validation loss aligning with shifts in the training data distribution at 600B and 1,100B tokens. Additionally, the validation loss exhibits initial fluctuations, stabilizing progressively with continued training. This trend indicates that the model increasingly adapts to the diversity of data types as training progresses.</p><p class='text'>Figure 2: Validation loss during the training process. The validation set consists of 5,000 documents including a diverse collection of English and Chinese texts sourced from a variety of data sources.</p><p class='text'>To our knowledge, the training of most prior LLMs utilized fully shuffling the training data, which was then fed into the model in a random sequence. Orion-14B is the first LLM trained with a specific data scheduling strategy. The evaluation results indicate that this model demonstrates impressive performance in language understanding tasks at its early stages and rapidly enhances its capabilities in reasoning and academic tasks in later stages, aligning with our data scheduling policy. Notably, Orion-14B, trained on 2.5T tokens, achieves comparable performance to other open-source models trained on 2.6T to 3T tokens, thereby illustrating the efficiency of our data utilization approach.</p><h2 class='section-header'>4 Fine-tuning</h2><p class='text'>During the pretraining stage, an LLM is trained to predict the next token at each step. However, in many applications, the model needs to generate a desired response to a given prompt. Thus, in the next stage, LLMs typically undergo further fine-tuning using supervised learning, where the training data consists of paired input and output text sequences. Further, to enhance the quality and safety, approaches like Reinforcement Learning from Human Feedback (RLHF)  or Direct Preference Optimization (DPO)  can be employed. In this work, our primary focus is on the supervised fine-tuning (SFT) stage, leaving RLHF and DPO for future exploration.</p><h2 class='section-header'>4.1 SFT Data</h2><p class='text'>High-quality, diverse data has been proven to be crucial to supervised fine-tuning in previous studies . To construct our SFT training data, we draw from two primary sources: a human-labeled dataset and an open-source filtered dataset.</p><p class='text'>For a high-quality human-labeled dataset, we assemble a team of expert annotators who spend weeks creating precisely annotated data. To ensure data quality, all annotators adhere to three key principles—helpfulness, truthfulness, and harmlessness—as outlined in InstructGPT  and LLaMA2 . In total, we produce approximately 220,000 human-labeled SFT data entries.</p><p class='text'>While the human-labeled dataset is of high quality, its volume is insufficient for training a high- performance LLM. Therefore, we also construct a large-scale, open-source filtered dataset. The original SFT data includes collections from various open-source datasets, such as COIG , WildChat , OpenOrca , and UltraChat . Given the variable quality and occasional presence of inappropriate content in these open-source datasets, we implement a cleaning process inspired by methods from Platypus  and MoDS , comprising the following steps:</p><p class='list-item'>• Rule-based filtering: We use regular expressions and keywords for simple filtering to remove personal information, temporal-sensitive data, etc.</p><p class='list-item'>• Quality filtering: A large NLP model scores the data quality on a scale from 1 to 10, retaining only data with a score of 7 or higher.</p><p class='list-item'>• Semantic deduplication: Text embeddings are used for semantic deduplication, considering texts with a similarity greater than 0.98 as duplicates.</p><p class='text'>Using this approach, we construct an open-source filtered dataset of 630,000 samples. Combined with the human-labeled data, we assemble an SFT dataset of 850,000 training pairs for supervised fine-tuning.</p><h2 class='section-header'>4.2 Training details</h2><p class='text'>To fine-tune a pretrained LLM, we prepend <human> and <assistant> as headers to the prompt text and the response text, respectively. The training process employs the AdamW optimizer, with hyperparameters configured as follows: β1 is set to 0.9, β2 to 0.95, and ϵ to 1e − 8. We limit the sequence length to 4096 and use a batch size of 128. Our training regimen spanned three epochs, involving over 500k samples; the learning rate was incrementally increased over the first 1,500 steps to a maximum of 1e − 5. To prevent overfitting, we apply a weight decay of 0.1, a dropout rate of 0.1, and a gradient clipping threshold of 1.0.</p><h2 class='section-header'>5 Evaluation 5.1 Standard Evaluation</h2><p class='text'>To effectively evaluate the LLM, we categorize the standard evaluation sets into the examinations and professional knowledge, and language understanding and common knowledge. We select the most common evaluation sets in each category as follows:</p><h2 class='section-header'>Professional Knowledge and Reasoning</h2><p class='list-item'>• C-Eval : A comprehensive Chinese evaluation benchmark consisting of more than 10,000 multi-choice questions.</p><p class='list-item'>• CMMLU : A general evaluation benchmark specifically designed to evaluate the knowledge and reasoning abilities of LLMs within the context of Chinese language and culture.</p><p class='list-item'>• MMLU : A widely used benchmark designed to measure knowledge acquired during pretraining by evaluating models.</p><p class='list-item'>• AGIEval : A human-centric benchmark crafted to assess the general capabilities of foundation models in tasks aligned with human cognition and problem- solving.</p><p class='list-item'>• Gaokao : A dataset leverages questions from China’s national college entrance examination to test LLMs.</p><p class='list-item'>• BBH : A challenging subset of the Big-Bench suite, covering a wide array of themes, such as linguistics, mathematics, common sense reasoning, biology, physics, software development, and more.</p><h2 class='section-header'>Language Understanding and Common Knowledge</h2><p class='list-item'>• RACE : A comprehensive reading comprehension dataset comprising over 28,000 passages and nearly 100,000 questions. It contains reading and comprehension materials for both middle school (middle) and high school (high) academic levels.</p><p class='list-item'>• HellaSwag : A challenge dataset for evaluating commonsense language inference that is particularly difficult for state-of-the-art models.</p><p class='list-item'>• PIQA : A dataset introducing the task of physical commonsense reasoning and a corresponding benchmark dataset.</p><p class='list-item'>• Lambada : A collection of narrative passages where human subjects can guess the last word if exposed to the whole passage, but not if they only see the last sentence preceding the target word.</p><p class='list-item'>• WSC : A pronoun disambiguation task, which requires determining the noun that the pronoun refers to according to the context.</p><p class='text'>For comparison, we select the most popular LLMs with a parameter range of 10-20 billion: LLaMA 2-13B , Skywork-13B , Baichuan 2-13B , Qwen-14B , InternLM .</p><p class='text'>To ensure consistent comparisons, we employ open-source LLM evaluation frameworks such as OpenCompass  and LM-Eval-Harness  for a unified perfor- mance evaluation of LLMs. For the models we compared, we refer to the published scores from OpenCompass or their official reports.</p><p class='caption'>Table 4: LLM evaluation results on examination and professional knowledge. Bold font denotes the best score in each category, a convention followed in all subsequent tables throughout this paper.</p><p class='text'>The evaluation results in Table 4 highlight Orion-14B’s superior performance across various examina- tions and professional knowledge evaluation sets, compared to other LLMs. Orion-14B achieves the highest scores in C-Eval, CMMLU, MMLU, AGIEval, and BBH, indicating its strong capabilities in understanding and reasoning within professional contexts. While it excels in most benchmarks, it is slightly outperformed by Qwen-14B in the Gaokao evaluation. These results position Orion-14B as a highly competitive and robust model for complex and professional tasks.</p><p class='caption'>Table 5: LLM evaluation results on language understanding and common knowledge.</p><p class='text'>As shown in Table 5, Orion-14B showcases robust performance in language understanding and common knowledge tasks, outperforming other models in RACE (mid and high), Lambada, and WSC benchmarks, highlighting its exceptional comprehension and reasoning abilities. However, for HellaSwag, PIQA, and WSC tasks, it is slightly outperformed by Qwen-14B and InternLM-20B. Overall, the results indicate Orion-14B’s strong capabilities across a spectrum of natural language understanding benchmarks.</p><p class='text'>For a comprehensive evaluation, we also utilize all test sets used in OpenCompass leaderboard  to assess performance. In OpenCompass leaderboard, the evaluation sets are organized into five categories. The summarized results for each category are shown in Table 6, where Orion-14B leads with an average score of 64.4%. Notably, it outperforms other models across four categories, including Examination, Language, Understanding, and Reasoning, indicating its excellent analytical and problem-solving abilities. These results demonstrate Orion-14B’s robust capabilities in a wide range of cognitive and language tasks. Detailed results for each testset are included in the Appendix B.</p><p class='text'>Note that, evaluation scores are not the definitive standard for assessing an LLM. Given the vast amount of training data, there is a high likelihood that the dataset includes elements of the evaluation set. To avoid this, we purposely deduplicate the evaluation datasets from our pretraining corpus, thereby ensuring that our model’s performance genuinely reflects its capabilities. Overlooking this critical step could lead to training a model that is overfitted to the evaluation set, resulting in artificially high scores. We will delve into this topic more deeply in Section 5.3.</p><h2 class='section-header'>5.2 Multilingual</h2><p class='text'>In our training approach, while the majority of the data is in English and Chinese, we also incorporate additional languages to enhance multilingual performance. Notably, Japanese and Korean texts are specifically added after surpassing 600B tokens in the training process. The total amounts of Japanese and Korean texts are approximately 100B and 50B tokens, respectively. Despite the lower quantity of Japanese and Korean tokens compared to English and Chinese, the model exhibits superior performance in these languages. This indicates a significant transfer of knowledge from the more dominant languages during the training of the LLM.</p><p class='text'>To assess the model’s multilingual capabilities, we benchmark it against other models trained on English+Japanese , English+Korean , or multilingual datasets . for evaluation of Japanese and Korean, respectively.</p><p class='caption'>Table 7: Comparison of LLM performances on Japanese testsets. The header of each column stands for Japanese CommonsenseQA, Japanese NLI, MARC in Japanese, Japanese SQUAD, Japanese QKET_v2, XLSUM in Japanese, XWinograd in Japanese, MGSM, respectively.</p><p class='caption'>Table 8: Comparison of LLM performances on Korean testsets. n = 0 and n = 5 stand for n-shot prompts used in the evaluation..</p><p class='caption'>Table 9: Multilingual evaluation.</p><p class='text'>As shown in Tables 7 and 8, Orion-14 consistently achieves the highest scores across the majority of the test sets. On average, it outperforms all other LLMs in Japanese and Korean datasets, surpassing even those models with a greater number of parameters.</p><p class='text'>To gain a clearer insight into the multilingual capabilities, we compute the average scores for the evaluation sets in Japanese, Korean, Chinese, and English for comparison. The scores for Japanese and Korean are derived directly from Tables 7 and 8. For the Chinese and English datasets, we calculate the average scores using the OpenCompass dataset, excluding the mathematics and programming testsets.</p><p class='text'>Table 9 demonstrates Orion-14B’s impressive performance in multilingual evaluations. It leads with top scores in Japanese, Korean, and Chinese, surpassing other multilingual models. In English, Orion-14B is marginally outperformed by Yi-34B, which is an LLM with a significantly higher number of parameters. This data highlights Orion-14B’s robust proficiency in multiple languages.</p><h2 class='section-header'>5.3 Analysis of Data Contamination</h2><p class='text'>The rise of the LLM has led to a surge in the performance of evaluation tasks. Their superior performance is primarily attributed to the massive data consumed by these billion/trillion-parameter LLMs during training. However, recent work  has shown that the performance of LLM on many downstream tasks may be inflated due to data contamination, i.e., the presence of test data from these downstream tasks in the pretraining data of LLMs.</p><p class='text'>As mentioned above, to prevent the pretraining dataset from containing answers to the evaluation datasets, we apply our deduplication approach using all evaluation datasets. This process removes text similar to the evaluation data from the training corpus. On the other hand, to understand the influence of such data, we also experiment with training a model using previously deduplicated data. Specifically, we select those data that had been removed due to deduplication with the evaluation set but we do not contain data with the exact same texts as in the evaluation texts. In other words, this approach allows us to use data that may be semantically or partially related to the evaluation set while excluding the exact text from it. We compile a smaller dataset of 200B tokens, which includes these selected data alongside the regular training data. We then continue the pretraining process with this 200B token dataset, resulting in a new pretrained model named Orion-14B-Exam. As illustrated in the accompanying table, Orion-14B-Exam demonstrates significantly higher scores on the evaluation set compared to the baseline.</p><p class='caption'>Table 10: Evaluation for data contamination and overfitting.</p><p class='text'>The results in Table 10 reveal that manipulating training data can easily lead to fitting the evaluation dataset and achieve very high scores. We conduct an additional experiment to gauge the extent of overfitting. Specifically, we gather a collection of recent texts from the Internet, ensuring they are unseen in any model’s training set. We then calculate the loss on this new dataset Lunseen and compare it to the loss on texts drawn from the evaluation sets Leval mentioned in Tables 4 and 5, including C-Eval, MMLU, HellaSwag, and others. The loss differential between these two sets serves as an indicator of overfitting—the smaller the difference, the lower the likelihood of overfitting to the evaluation set. The results of this analysis are presented in Table 11. This table illustrates that with the inclusion of the new training dataset, there is a significant reduction in the loss on the evaluation set, decreasing from 1.87 to 1.44, clearly showing the overfitting on the evaluation set. On the other hand, the original Orion-14B model demonstrates consistent losses on both datasets, with a minimal difference as expected, indicating little overfitting levels.</p><p class='caption'>Table 11: Overfitting analysis of the loss of each model.</p><p class='text'>In light of these performance, it is crucial to examine the evaluation methods used in the community of LLM. Since it is possible to achieve high scores through specific training tactics, such scores may not accurately reflect the true capabilities of an LLM. An overemphasis on top leaderboard positions can be misleading and does not guarantee actual model proficiency. The principal goal should be to develop robust, effective LLMs that prove their utility in a wide range of real-world applications.</p><h2 class='section-header'>5.4 Fine-tuned Model Evaluations</h2><p class='text'>The above evaluation utilizes standard evaluation datasets to test the performance of the pretrained foundation model (base-model). On the other hand, evaluating the performance of the fine-tuned model (chat-model) differs from that of the base-model. This is because the chat-model is designed to generate responses to given prompts, and determining the goodness of these responses can be subjective and dependent on the specific task. To comprehensively evaluate the chat-model’s performance, we conduct tests using three different approaches: 1) standard evaluation sets, similar to those used in the base-model evaluation; 2) subjective datasets based on GPT-4 scoring; and 3) human evaluation.</p><p class='text'>For the standard evaluation, we use widely recognized benchmarks, including CMMLU, MMLU, BBH, HellaSwag, PIQA, and WSC. As indicated in 12, Orion-14B-chat maintains strong performance in HellaSwag, BBH, PIQA, and WSC. However, there is a slight decline in performance on CMMLU and MMLU compared to the base model in Tabels 4 and 5. This is likely due to the evaluation prompts being more designed for the base model than the chat model. Therefore, incorporating subjective evaluation methods alongside standard metrics could provide a more comprehensive assessment of</p><p class='caption'>Table 12: Standard evaluation for chat models.</p><p class='text'>the model’s capabilities. We utilize MT-Bench  and AlignBench  for English and Chinese, respectively.</p><p class='caption'>Table 13: Subjective evaluation of MT-Bench.</p><p class='caption'>Table 14: Subjective evaluation of AlignBench. The header of each column stands for Mathematics, Logic, Basic tasks, Chinese understanding, Comprehensive Q&A, Writing, Role-playing, and Profes- sional tasks, and Average scores.</p><p class='text'>The results presented in Tables 13 and 14 highlight Orion-14B-Chat’s performance in subjective evaluations. In MT-Bench evaluation, Orion-14B-Chat significantly outperforms other models, achieving the highest scores in both First-Turn and Second-Turn evaluations, with an average score of 7.37. In the AlignBench evaluation, Orion-14B-Chat excels notably in Chinese understanding, Writing, Role-Playing, and Professional tasks. The results demonstrate competitive performance across diverse conversational contexts.</p><p class='text'>As the chat model is designed to generate responses to prompts, human evaluation is a critical measure of its effectiveness. Adopting an approach similar to the arena method used in Chatbot Arena , we engage human annotators to assess responses from two models in a randomized head-to-head format. Specifically, for a given prompt, responses generated by two anonymized models are presented to the annotators, who then rate them as "Win," "Tie," or "Loss" based on their preference. We have 14 human annotators evaluate a total of 3562 questions. The models compared in this arena battle are Orion-14B-Chat, Qwen-14B-Chat, and Baichuan2-13B-Chat. As indicated in Table 15, Orion-14B-Chat received the highest number of "win" votes, highlighting its exceptional performance in human evaluations.</p><h2 class='section-header'>6 Extension Works</h2><p class='text'>In practical applications, LLMs have a variety of needs, including extended context handling, minimizing inference resource requirement, and adapting to specific applications. To address these challenges, we conduct extension works and develop several specialized models. Below are the extensions we have implemented:</p><p class='list-item'>• Orion-14B-Long: This model is optimized for long context lengths more than 200,000 tokens and demonstrates performance comparable to proprietary models on long context evaluation sets .</p><p class='caption'>Table 15: Human arena evaluation for chat models.</p><p class='text'>Orion-14B-Chat 1172 1491 899 Qwen-14B-Chat 1101 1592 869 Baichuan2-13B-Chat 728 1601 1233</p><p class='list-item'>• Orion-14B-INT4: A quantized model utilizing 4-bit integer weights. It significantly reduces the model size by 70% and increases the inference speed by 30% while incurring a minimal performance loss of only 1%.</p><p class='list-item'>• Orion-14B-RAG: A chat-model fine-tuned on a custom retrieval augmented generation dataset, achieving superior performance in retrieval augmented generation tasks.</p><p class='list-item'>• Orion-14B-PlugIn: A chat-model specifically tailored for plugin and function calling tasks, ideal for agent-related scenarios where the LLM acts as a plugin and function call system.</p><p class='text'>Due to time constraints, this technical report does not cover the training details and evaluations of these models. We make all the above models available for public use. For more information, please refer to our open-source library https://github.com/OrionStarAI/Orion.</p><h2 class='section-header'>7 Conclusion</h2><p class='text'>In this study, we present Orion-14B, a diverse suite of multilingual large language models with 14 billion (14B) parameters. This family includes a pretrained base model and a fine-tuned chat model, as detailed in this technical report. Additionally, we offer several extensions to Orion-14B, such as a long context model, a quantized model, and several application-oriented models, enhancing its versatility and applicability. These models have demonstrated competitive performance against existing open-source models in the field of LLMs, positioning Orion-14B as a potential strong baseline for future LLM research.</p><p class='text'>Training a large language model like Orion-14B poses considerable challenges. Throughout this endeavor, we faced numerous obstacles and overcame significant hurdles. We responsibly provide open access to the Orion-14B family and documented our experiences and insights in this technical report, aiming to assist and inspire other researchers in the community.</p>
    </body>
    </html>
    